{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":24286,"databundleVersionId":1878097,"sourceType":"competition"},{"sourceId":1499994,"sourceType":"datasetVersion","datasetId":881740},{"sourceId":2011028,"sourceType":"datasetVersion","datasetId":1202205},{"sourceId":56338708,"sourceType":"kernelVersion"}],"dockerImageVersionId":30068,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"## CLIP with fine tune\n","metadata":{}},{"cell_type":"code","source":"%%capture\n\n\n\nimport sys\n!cp -r ../input/openai-clip/CLIP/CLIP-main /tmp/\n\n!gzip -c /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt > /tmp/CLIP-main/clip/bpe_simple_vocab_16e6.txt.gz\nsys.path.append('/tmp/CLIP-main')\n\n!pip install ../input/openai-clip/ftfy-5.9/ftfy-5.9 \\\n             ../input/openai-clip/torch-1.7.1+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/openai-clip/torchvision-0.8.2+cu110-cp37-cp37m-linux_x86_64.whl \\\n             ../input/faiss-163/faiss_gpu-1.6.3-cp37-cp37m-manylinux2010_x86_64.whl","metadata":{"_kg_hide-input":true,"_kg_hide-output":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\nimport torch\nfrom torch.utils.data import Dataset, DataLoader\nfrom torch.utils.data.sampler import Sampler\nimport clip\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm.auto import tqdm\nimport re\nfrom clip.simple_tokenizer import SimpleTokenizer\nimport faiss\nimport matplotlib.pyplot as plt\nfrom triplet_loss import TripletLoss\n\n%matplotlib inline","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_test = pd.read_csv('../input/shopee-product-matching/test.csv', index_col='posting_id')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"RUN_ON_TRAIN = len(df_test) == 3","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Utility classes and functions","metadata":{}},{"cell_type":"code","source":"_tokenizer = SimpleTokenizer()\n\n# 来自 https://github.com/openai/CLIP/blob/beba48f35392a73c6c47ae67ddffced81ad1916d/clip/clip.py#L164\ndef tokenize(texts, context_length: int = 77) -> torch.LongTensor:\n    if isinstance(texts, str):\n        texts = [texts]\n\n    sot_token = _tokenizer.encoder[\"<|startoftext|>\"]\n    eot_token = _tokenizer.encoder[\"<|endoftext|>\"]\n    all_tokens = [[sot_token] + _tokenizer.encode(text) + [eot_token] for text in texts]\n    result = torch.zeros(len(all_tokens), context_length, dtype=torch.long)\n\n    for i, tokens in enumerate(all_tokens):\n        n = min(len(tokens), context_length)\n        result[i, :n] = torch.tensor(tokens)[:n]\n        if len(tokens) > context_length:\n            result[i, -1] = tokens[-1]\n\n    return result","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Remove EMOJI\nRE_EMOJI = re.compile(r\"\\\\x[A-Za-z0-9./]+\", flags=re.UNICODE)\n\ndef strip_emoji(text):\n    return RE_EMOJI.sub(r'', text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RollingMean():\n    def __init__(self):\n        self.n = 0\n        self.mean = 0\n        \n    def update(self, value):\n        self.mean = (self.mean * self.n + value) / (self.n+1)\n        self.n += 1\n        \n    def result(self):\n        return self.mean","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 数据集和采样器\n\n确保在每个批次中始终存在同一组的两个样本。","metadata":{}},{"cell_type":"code","source":"class SameGroupSampler(Sampler):\n    def __init__(self, df ,ds):\n        super().__init__(ds)\n        \n        # Create a dictionary of posting_id -> index in dataset\n        self.index_to_position = dict(zip(df.index, range(len(df))))\n        \n        # Create a Series of label_group -> set(posting_id)\n        self.label_group = df.reset_index().groupby('label_group')['posting_id'].apply(set).map(sorted).map(np.array)\n\n    def __len__(self):\n        return len(self.label_group)\n        \n    def __iter__(self):\n        for _ in range(len(self)):\n            # Sample one label_group\n            label_group_sample = self.label_group.sample(1).iloc[0]\n            \n            # Sample two posting_id's\n            sample1, sample2 = np.random.choice(label_group_sample, 2, replace=False)\n            \n            yield self.index_to_position[sample1]\n            yield self.index_to_position[sample2]            ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MyDataset(Dataset):\n    def __init__(self, df, images_path):\n        super().__init__()\n        self.df = df\n        self.images_path = images_path\n        self.has_target = ('label_group' in df)\n        \n    def __len__(self):\n        return len(self.df)\n    \n    def __getitem__(self, idx):\n        row = self.df.iloc[idx]\n        \n        image = preprocess(Image.open(self.images_path / row['image']))\n        text = tokenize([strip_emoji(row['title'])])[0]\n        \n        if self.has_target:\n            return image, text, row['label_group']\n        else:\n            return image, text, 0","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### 微调","metadata":{}},{"cell_type":"code","source":"# 加载 CLIP\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nmodel, preprocess = clip.load(\"../input/openai-clip/ViT-B-32.pt\", device=device, jit=False)\n\n# 获取嵌入大小\nembed_dim = model.text_projection.shape[1]\nembed_dim","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# 训练数据\ntrain_images_path = Path('../input/shopee-product-matching/train_images')\n\ndf_train = pd.read_csv('../input/shopee-product-matching/train.csv', index_col='posting_id')\n\ndstrain = MyDataset(df_train, train_images_path)\ndltrain = DataLoader(dstrain, batch_size=128, num_workers=2, sampler=SameGroupSampler(df_train, dstrain))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"n_epochs = 1","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# optim = torch.optim.AdamW(model.parameters(), lr=1e-4, eps=1e-8, weight_decay=1e-2)\noptim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.2)\nscheduler = torch.optim.lr_scheduler.OneCycleLR(optim, 1e-2, total_steps=n_epochs * (2*len(dltrain)-1),\n                                               base_momentum=0.0, max_momentum=0.5, pct_start=0.1, div_factor=1e2, final_div_factor=1e4)\ncriterion = TripletLoss(device)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"for epoch in range(n_epochs):\n    with tqdm(total=2*len(dltrain)-1) as bar:\n        loss_mean = RollingMean()\n        for images, texts, targets in dltrain:\n            targets = targets.to(device)\n            \n            # 图文特征\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n            optim.zero_grad()\n\n            # 图文特征融合\n            features = torch.hstack([images_features, texts_features])\n            \n            # L2 规范化\n            features = features / features.norm(2, dim=1, keepdim=True)\n\n            # 应用 Triplet SemiHardLoss\n            loss = criterion(features, targets)\n\n            loss.backward()\n            optim.step()\n            scheduler.step()\n\n            # 进度条\n            loss_mean.update(loss.item())\n            bar.update()\n            bar.set_description('{:.4f}'.format(loss_mean.result()))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## 在训练集上运行\n\n使用 CLIP 生成特征并执行相似性搜索以查找最接近的match。\n\n通过去掉那些低于阈值（0.7）的数据的结果来创建结果集","metadata":{}},{"cell_type":"code","source":"def find_similarities_and_indexes(df, images_path, top_n=100, features_file=None):\n    # 创建 pytorch Dataset/DataLoader\n    ds = MyDataset(df, images_path)\n    dl = DataLoader(ds, batch_size=32, shuffle=False, num_workers=2)\n\n    \n    features = np.empty((len(df), 2*embed_dim), dtype=np.float32)\n\n    # 开始预测\n    i = 0\n    for images, texts, _ in tqdm(dl):\n        n = len(images)\n        with torch.no_grad():\n            # Generate image and text features\n            images_features = model.encode_image(images.to(device))\n            texts_features = model.encode_text(texts.to(device))\n\n        # 拼接特征\n        features[i:i+n, :embed_dim] = images_features.cpu()\n        features[i:i+n, embed_dim:] = texts_features.cpu()\n\n        i += n\n\n    # 保存特征\n    if features_file is not None:\n        np.save(features_file, features)\n\n    # L2 规范化\n    features /= np.linalg.norm(features, 2, axis=1, keepdims=True)\n\n    # 创建 index\n    index = faiss.IndexFlatIP(2*embed_dim)\n    index.add(features)\n\n    # 搜索 index\n    return index.search(features, top_n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_ON_TRAIN:\n    # 执行相似商品的搜索\n    similarities, indexes = find_similarities_and_indexes(df_train, train_images_path, features_file='features-no-norm.npy')\n    \n    # `similarities`  (n, 100) 具有最接近匹配的相似分数\n    # `indexes` (n, 100) 具有最接近匹配的索引。\n    # 两个数组都是对齐的\n\n    # 将索引转换为组，形状为 (n, 100)\n    found_groups = df_train['label_group'].values[indexes]\n\n    # 检查匹配是否来自同一组\n\n    # 绘制同一组和不同组的相似度得分\n    plt.figure(figsize=(10, 5))\n    plt.hist([similarities[is_same_group], similarities[~is_same_group]], density=False, bins=51,\n         label=['Same group', 'Different group'], histtype='stepfilled', alpha=0.75)\n    plt.xlim(0, 1)\n    plt.xlabel('Similarity score')\n    plt.legend();","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Tune CUT\n\nIn this last step we will move the `cut_value` to find optimal F1-score.","metadata":{}},{"cell_type":"code","source":"def row_wise_f1_score(y_true, y_pred):\n    tp = np.array([len(x[0] & x[1]) for x in zip(y_true, y_pred)])\n    fp = y_pred.apply(lambda x: len(x)).values - tp\n    fn = y_true.apply(lambda x: len(x)).values - tp\n\n    precision = tp / (tp + fp)\n    recall = tp / (tp + fn)\n    f1 = 2 * ((precision * recall) / (precision + recall))\n    return f1\n\n\ndef calc_score(cut_value):\n    # 应用相似性截断\n    groups_are_same = (similarities > cut_value)\n\n    # 创建结果集\n    results = []\n    for i, (group_is_same, index_result) in enumerate(zip(groups_are_same, indexes)):\n        row_results = df_train.index[index_result[group_is_same]]\n\n        results.append(set(row_results))\n\n    df_results = pd.Series(results, index=df_answer.index)\n    \n    # 计算F1 得分\n    return row_wise_f1_score(df_answer, df_results).mean()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"if RUN_ON_TRAIN:\n    groups = df_train.reset_index().groupby('label_group')['posting_id'].apply(set)\n    df_answer = df_train['label_group'].map(groups)\n\n\n    cuts = np.linspace(0.5, 0.95, 51)\n    scores = [calc_score(c) for c in tqdm(cuts)]\n\n\n    plt.plot(cuts, scores)\n    plt.xlabel('Cutoff value')\n    plt.ylabel('F1 score')\n\n    print('Best cutoff is {:.2f} with expected F1 score of {:.4f}'.format(cuts[np.argmax(scores)], max(scores)))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Run on test","metadata":{}},{"cell_type":"code","source":"GROUP_CUT = 0.71 ","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_images_path = Path('../input/shopee-product-matching/test_images')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Find similar matches\nsimilarities, indexes = find_similarities_and_indexes(df_test, test_images_path)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Apply cutoff of similiarites\ntest_are_same_groups = (similarities > GROUP_CUT)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Build submission\nresults = []\n\nfor i, (test_is_same_group, index_result) in enumerate(zip(test_are_same_groups, indexes)):\n    row_results = set(df_test.index[index_result[test_is_same_group]])\n    \n    results.append({\n        'posting_id': df_test.index[i],\n        'matches': ' '.join(row_results)\n    })\n    \ndf_sub = pd.DataFrame(results)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df_sub.to_csv('submission.csv', index=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}